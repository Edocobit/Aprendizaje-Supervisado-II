{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32349,"status":"ok","timestamp":1768301769024,"user":{"displayName":"Guido Rios","userId":"10562185620794585819"},"user_tz":180},"id":"8J8DEjeL4b3o","outputId":"c29f8a29-fcf2-48c3-ef3a-23da106bcf5e"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Rama detectada: main\n","\n","$ git init\n","\n","$ git remote add origin \"https://github.com/GuidoRiosCiaffaroni/Machine_Learning_II.git\"\n","\n","$ git sparse-checkout init --cone\n","\n","$ git sparse-checkout set \"DataImg/TomatoDataset_ready\"\n","\n","$ git pull --depth 1 origin \"main\"\n","\n"," Proceso finalizado. Carpeta lista en: /content/DataImg/TomatoDataset_ready\n"," Archivos (primeros 30):\n","['Tomato___Late_blight', 'Tomato___Septoria_leaf_spot', 'Tomato___Tomato_mosaic_virus', 'Tomato___Target_Spot', 'Tomato___Leaf_Mold', 'Tomato___Bacterial_spot', 'Tomato___healthy', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Early_blight', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus']\n"]}],"source":["# ============================================\n","# 1\n","# Descargar SOLO una carpeta desde GitHub en Google Colab (Sparse Checkout)\n","# Deja la carpeta lista en /content/DataImg\n","# ============================================\n","\n","import os\n","import shutil\n","import subprocess\n","\n","# 1) Variables\n","REPO_URL = \"https://github.com/GuidoRiosCiaffaroni/Machine_Learning_II.git\"\n","FOLDER_NAME = \"DataImg/TomatoDataset_ready\"\n","TMP_DIR = \"/content/repo_temporal\"\n","\n","# 2) Helpers\n","def run(cmd, cwd=None):\n","    print(f\"\\n$ {cmd}\")\n","    subprocess.check_call(cmd, shell=True, cwd=cwd)\n","\n","def get_default_branch(repo_url):\n","    # Detecta rama por defecto (main / master)\n","    try:\n","        out = subprocess.check_output(\n","            f'git ls-remote --symref {repo_url} HEAD',\n","            shell=True,\n","            text=True\n","        )\n","        for line in out.splitlines():\n","            if line.startswith(\"ref:\"):\n","                return line.split(\"refs/heads/\")[-1].split(\"\\t\")[0].strip()\n","    except Exception:\n","        pass\n","    return \"main\"\n","\n","# 3) Preparación: limpiar previos\n","shutil.rmtree(TMP_DIR, ignore_errors=True)\n","shutil.rmtree(f\"/content/{FOLDER_NAME}\", ignore_errors=True)\n","os.makedirs(TMP_DIR, exist_ok=True)\n","\n","# 4) Detectar rama por defecto\n","BRANCH = get_default_branch(REPO_URL)\n","print(f\" Rama detectada: {BRANCH}\")\n","\n","# 5) Inicializar repo temporal y configurar sparse checkout\n","run(\"git init\", cwd=TMP_DIR)\n","run(f'git remote add origin \"{REPO_URL}\"', cwd=TMP_DIR)\n","run(\"git sparse-checkout init --cone\", cwd=TMP_DIR)\n","run(f'git sparse-checkout set \"{FOLDER_NAME}\"', cwd=TMP_DIR)\n","\n","# 6) Descargar solo esa carpeta\n","run(f'git pull --depth 1 origin \"{BRANCH}\"', cwd=TMP_DIR)\n","\n","# 7) Mover a /content y limpiar\n","src_path = os.path.join(TMP_DIR, FOLDER_NAME)\n","dst_path = os.path.join(\"/content\", FOLDER_NAME)\n","\n","if not os.path.exists(src_path):\n","    raise FileNotFoundError(\n","        f\"No se encontró la carpeta '{FOLDER_NAME}' en el repo. \"\n","        f\"Revisa que exista en la rama '{BRANCH}'.\"\n","    )\n","\n","shutil.move(src_path, dst_path)\n","shutil.rmtree(TMP_DIR, ignore_errors=True)\n","\n","print(f\"\\n Proceso finalizado. Carpeta lista en: {dst_path}\")\n","print(\" Archivos (primeros 30):\")\n","print(os.listdir(dst_path)[:30])\n"]},{"cell_type":"markdown","source":["# Colab — KNN para enfermedades del tomate (con embeddings CNN)"],"metadata":{"id":"carjXVab_1VZ"}},{"cell_type":"markdown","source":["# Bloque 1 — Imports y reproducibilidad"],"metadata":{"id":"7_6mTgHM_3sx"}},{"cell_type":"markdown","source":["\n","\n","## Objetivo del bloque\n","Este bloque prepara el **entorno de trabajo** para el proyecto de clasificación de imágenes, asegurando que:\n","1. Las **librerías necesarias** estén disponibles (manejo de archivos, análisis de datos, Deep Learning y Machine Learning).\n","2. La ejecución sea **reproducible**, es decir, que al correr el notebook múltiples veces se obtengan resultados **consistentes** (dentro de lo posible).\n","\n","---\n","\n","## Paso 1: Importación de librerías base (sistema y utilidades)\n","Se cargan librerías de propósito general para:\n","- **Navegar por directorios y manejar rutas** (útil para datasets organizados en carpetas por clase).\n","- **Listar archivos de imágenes** con patrones de búsqueda (por ejemplo, `*.jpg`, `*.png`).\n","- Controlar la **aleatoriedad** del proceso experimental (por ejemplo, particiones de datos).\n","\n","**Relevancia:** en clasificación de imágenes con estructura “carpetas = clases”, este conjunto de herramientas es esencial para construir un índice de imágenes y etiquetas.\n","\n","---\n","\n","## Paso 2: Importación de librerías científicas y de análisis\n","Se importan herramientas estándar para:\n","- **Cálculo numérico** y operaciones vectorizadas (base para features, embeddings y métricas).\n","- **Manipulación tabular** de datos (ideal para construir un *dataframe* que contenga `path` de la imagen y `label`).\n","\n","**Relevancia:** aunque el problema sea de imágenes, mantener un registro estructurado en forma de tabla facilita:\n","- auditoría del dataset,\n","- conteo por clase,\n","- detección de desbalance,\n","- trazabilidad del pipeline.\n","\n","---\n","\n","## Paso 3: Importación de TensorFlow (Deep Learning)\n","Se incorpora TensorFlow como framework para:\n","- Cargar imágenes eficientemente (por lotes).\n","- Aplicar preprocesamiento y/o extracción de características (embeddings) usando redes preentrenadas.\n","- Estandarizar el flujo de trabajo de visión por computador en Colab.\n","\n","**Relevancia:** TensorFlow permite manejar imágenes a escala y aprovechar aceleración por GPU.\n","\n","---\n","\n","## Paso 4: Importación de herramientas de scikit-learn (ML clásico y evaluación)\n","Se importan módulos clave para construir un pipeline reproducible y evaluable:\n","\n","### 4.1 Partición y validación\n","- División de datos en conjuntos de entrenamiento y prueba.\n","- Estrategias de validación cruzada estratificada (importante en clasificación multiclase desbalanceada).\n","- Búsqueda sistemática de hiperparámetros (*Grid Search*).\n","\n","**Relevancia:** asegura una evaluación correcta de generalización y evita conclusiones basadas en un solo split.\n","\n","### 4.2 Preprocesamiento\n","- Codificación de etiquetas (transformar nombres de clases a números).\n","- Estandarización de características (crítica para algoritmos basados en distancia como KNN).\n","\n","**Relevancia:** KNN depende directamente de distancias; si las variables no están en escalas comparables, el modelo se sesga.\n","\n","### 4.3 Pipeline\n","Se utiliza un enfoque de “pipeline” para encadenar:\n","- transformaciones (ej. escalamiento),\n","- modelo (KNN),\n","de forma consistente y sin fugas de información (*data leakage*).\n","\n","**Relevancia:** mejora buenas prácticas y hace el experimento replicable y ordenado.\n","\n","### 4.4 Métricas\n","Se incluyen métricas para evaluación robusta:\n","- **Accuracy** (visión global).\n","- **F1-score** (balance precisión/recall, especialmente útil en desbalance).\n","- **Reporte por clase** (precision, recall, F1 y soporte).\n","- **Matriz de confusión** (diagnóstico de confusiones entre enfermedades).\n","\n","**Relevancia:** en problemas multiclase de enfermedades, es crucial analizar **qué clases se confunden** para justificar mejoras.\n","\n","---\n","\n","## Paso 5: Definición de semilla y control de aleatoriedad (reproducibilidad)\n","Se fija una semilla global (`SEED = 42`) y se aplica a:\n","- Generador aleatorio estándar,\n","- generador de NumPy,\n","- generador de TensorFlow.\n","\n","**Por qué es importante:**\n","- Reduce variabilidad entre ejecuciones.\n","- Permite reproducir splits, resultados de entrenamiento y comparaciones de modelos de forma más confiable.\n","- Es un requisito típico en contextos académicos y profesionales.\n","\n","> Nota metodológica: en Deep Learning, la reproducibilidad total puede depender también del hardware (GPU) y de operaciones no deterministas, pero fijar semillas sigue siendo una práctica esencial.\n","\n","---\n","\n","## Paso 6: Verificación del entorno\n","Finalmente, se imprime una confirmación de que el entorno está listo, incluyendo la versión de TensorFlow.\n","\n","**Relevancia:**\n","- Asegura trazabilidad (por ejemplo, para comparar resultados entre notebooks o máquinas).\n","- Es útil para depuración y documentación del informe.\n","\n","---\n","\n","## Resultado del bloque\n","Al finalizar este bloque, el notebook queda preparado para:\n","1. Indexar el dataset de imágenes desde carpetas.\n","2. Preprocesar etiquetas y particionar datos de manera consistente.\n","3. Entrenar modelos con evaluación y ajuste de hiperparámetros.\n","4. Mantener un flujo de trabajo reproducible y defendible.\n"],"metadata":{"id":"Voo_FZ85WL55"}},{"cell_type":"code","source":["# ============================================\n","# Bloque 1: Imports y reproducibilidad\n","# ============================================\n","\n","import os\n","import glob\n","import random\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","\n","from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import (\n","    classification_report, confusion_matrix,\n","    accuracy_score, f1_score\n",")\n","\n","SEED = 42\n","random.seed(SEED)\n","np.random.seed(SEED)\n","tf.random.set_seed(SEED)\n","\n","print(\"✅ Entorno listo. TF:\", tf.__version__)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ORpO7xUu_6jp","executionInfo":{"status":"ok","timestamp":1768302066085,"user_tz":180,"elapsed":51,"user":{"displayName":"Guido Rios","userId":"10562185620794585819"}},"outputId":"751fbb2b-48e7-4127-d620-80f4a056b36d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Entorno listo. TF: 2.19.0\n"]}]},{"cell_type":"markdown","source":["# Bloque 2 — Ruta del dataset y detección de clases"],"metadata":{"id":"a9MYacE-_-2h"}},{"cell_type":"markdown","source":["\n","\n","## Objetivo del bloque\n","Este bloque establece el **punto de entrada al conjunto de datos** y construye el listado oficial de **clases** del problema de clasificación, asumiendo una estructura supervisada típica:\n","\n","- **Una carpeta por clase** (enfermedad o estado saludable).\n","- Las imágenes correspondientes se encuentran dentro de cada carpeta.\n","\n","El resultado es un vector `class_names` que actúa como referencia para:\n","- la codificación de etiquetas,\n","- la creación del índice de imágenes,\n","- la interpretación de métricas por clase (reporte y matriz de confusión),\n","- y la consistencia del pipeline completo.\n","\n","---\n","\n","## Paso 1: Definición de la ruta raíz del dataset\n","Se define una variable que apunta al directorio donde está almacenado el dataset en Google Colab.\n","\n","**Relevancia práctica**\n","- Centraliza la configuración: si cambia la ubicación del dataset, basta modificar esta línea.\n","- Facilita reutilización del notebook y evita “rutas duras” dispersas en el código.\n","\n","**Relevancia metodológica**\n","- Permite que el pipeline sea reproducible: el dataset se localiza desde un único punto controlado.\n","\n","---\n","\n","## Paso 2: Validación temprana de existencia del directorio\n","Se utiliza una verificación inmediata (assert) para asegurar que la ruta definida **existe realmente**.\n","\n","**Por qué es importante**\n","- Evita errores posteriores más difíciles de depurar (por ejemplo, listas vacías o fallas al leer imágenes).\n","- Implementa una buena práctica de ingeniería: *fail fast* (fallar temprano si una condición crítica no se cumple).\n","- Asegura que el notebook sea autocontenido: informa claramente cuándo el problema es de ruta/montaje y no del modelo.\n","\n","---\n","\n","## Paso 3: Identificación automática de clases a partir de carpetas\n","Se recorre el contenido del directorio raíz y se seleccionan únicamente los elementos que son **subdirectorios**, ya que cada uno representa una clase del problema.\n","\n","**Qué significa esto en términos de ML supervisado**\n","- Cada carpeta funciona como la “etiqueta” de sus imágenes.\n","- Por ejemplo, una carpeta `Tomato___Late_blight` representa la clase *Late blight*.\n","- Esta convención es estándar en clasificación de imágenes y se alinea con loaders comunes en frameworks de visión.\n","\n","---\n","\n","## Paso 4: Ordenamiento de clases para consistencia\n","El listado de clases se ordena alfabéticamente.\n","\n","**Por qué el orden importa**\n","- Garantiza que el mapeo clase → índice sea **estable** entre ejecuciones.\n","- Evita inconsistencias del tipo: “la clase 0 cambió” entre distintos runs o notebooks.\n","- Asegura trazabilidad en reportes y matrices de confusión, donde el orden de clases debe ser fijo.\n","\n","---\n","\n","## Paso 5: Reporte de control (sanity check)\n","Finalmente, se imprime:\n","- el número total de clases detectadas,\n","- y el listado completo de nombres de clases.\n","\n","**Relevancia**\n","- Confirma que el dataset se está interpretando correctamente.\n","- Permite detectar problemas frecuentes:\n","  - carpetas extra (por ejemplo, `.ipynb_checkpoints`),\n","  - errores de estructura (clases dentro de una carpeta adicional),\n","  - o rutas mal definidas.\n","\n","---\n","\n","## Resultado del bloque\n","Al finalizar este bloque se obtiene:\n","1. Un **directorio raíz validado** (`DATA_ROOT`) desde donde se leerán las imágenes.\n","2. Un listado consistente y ordenado de **clases** (`class_names`), que será la base para:\n","   - construir el dataset (`path`, `label`),\n","   - codificar etiquetas numéricas,\n","   - y evaluar el desempeño por clase de forma correcta.\n"],"metadata":{"id":"D9VQApfkWdHR"}},{"cell_type":"code","source":["# ============================================\n","# Bloque 2: Ruta del dataset y clases\n","# ============================================\n","\n","DATA_ROOT = \"/content/DataImg/TomatoDataset_ready\"  # ajusta si tu ruta difiere\n","\n","assert os.path.exists(DATA_ROOT), f\"❌ No existe la ruta: {DATA_ROOT}\"\n","\n","class_names = sorted([d for d in os.listdir(DATA_ROOT) if os.path.isdir(os.path.join(DATA_ROOT, d))])\n","print(\"✅ Clases encontradas:\", len(class_names))\n","print(class_names)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xrp4vnhDABrq","executionInfo":{"status":"ok","timestamp":1768302098700,"user_tz":180,"elapsed":45,"user":{"displayName":"Guido Rios","userId":"10562185620794585819"}},"outputId":"a9c9e257-6062-46f3-ddd7-c9b7cf461f37"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Clases encontradas: 10\n","['Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy']\n"]}]},{"cell_type":"markdown","source":["# Bloque 3 — Indexación de imágenes por clase"],"metadata":{"id":"H0SeNypYAN7i"}},{"cell_type":"markdown","source":["\n","\n","## Objetivo del bloque\n","Este bloque construye el **catálogo estructurado del dataset** a partir de la organización “carpeta = clase”.  \n","En términos prácticos, transforma una colección de archivos (imágenes distribuidas en subcarpetas) en una tabla con dos columnas fundamentales:\n","\n","- **`path`**: ruta completa del archivo de imagen.\n","- **`label`**: nombre de la clase asociada (la carpeta donde está la imagen).\n","\n","Este paso es esencial porque permite tratar el dataset como un conjunto de datos supervisado estándar, facilitando el preprocesamiento, el *split* estratificado, la auditoría del desbalance y la trazabilidad del pipeline.\n","\n","---\n","\n","## Paso 1: Definición de extensiones válidas de imagen\n","Se establece un conjunto de extensiones admitidas (por ejemplo `.jpg`, `.jpeg`, `.png`) para filtrar únicamente archivos que correspondan a imágenes.\n","\n","**Por qué es importante**\n","- Evita incluir archivos no deseados (por ejemplo, `txt`, `json`, miniaturas o archivos del sistema).\n","- Asegura que el pipeline de lectura posterior no falle por tipos de archivo incompatibles.\n","\n","---\n","\n","## Paso 2: Inicialización de contenedores para rutas y etiquetas\n","Se crean dos listas vacías:\n","- una para acumular las **rutas de las imágenes**,\n","- otra para acumular la **etiqueta (clase)** de cada imagen.\n","\n","**Interpretación supervisada**\n","- Cada elemento de `image_paths` representa una instancia \\(x_i\\).\n","- Cada elemento correspondiente en `labels` representa la etiqueta \\(y_i\\).\n","- En conjunto, se construye el mapeo \\((x_i, y_i)\\) requerido por aprendizaje supervisado.\n","\n","---\n","\n","## Paso 3: Recorrido clase por clase (carpeta por carpeta)\n","Se itera sobre el listado de clases detectadas previamente. Para cada clase:\n","\n","1. Se construye la ruta de su carpeta.\n","2. Se buscan todos los archivos de imagen dentro de esa carpeta que coincidan con las extensiones válidas.\n","3. Se agregan las rutas encontradas al catálogo global.\n","4. Se generan las etiquetas correspondientes repitiendo el nombre de la clase tantas veces como imágenes se encontraron.\n","\n","**Fundamento**\n","- La **etiqueta** se deriva de la carpeta: esto implementa la convención estándar en visión por computador para clasificación supervisada.\n","- Se conserva trazabilidad: cada imagen queda asociada explícitamente a una clase.\n","\n","---\n","\n","## Paso 4: Creación de una tabla estructurada (DataFrame)\n","Las dos listas (rutas y etiquetas) se convierten en una tabla de datos con dos columnas:\n","\n","- `path`: ubicaciones de los archivos\n","- `label`: clase de cada imagen\n","\n","**Ventajas metodológicas**\n","- Permite aplicar operaciones de análisis exploratorio (conteos, muestreos, filtros).\n","- Facilita el *split* estratificado porque las etiquetas quedan explícitas.\n","- Ayuda a detectar errores en el dataset (clases vacías, rutas inválidas, etc.).\n","\n","---\n","\n","## Paso 5: Verificación inicial del dataset (sanity check)\n","Se imprime el **número total de imágenes** y se muestra una vista previa de las primeras filas.\n","\n","**Por qué es importante**\n","- Confirma que el índice fue construido correctamente.\n","- Permite verificar rápidamente:\n","  - rutas bien formadas,\n","  - correspondencia correcta entre rutas y etiquetas,\n","  - y ausencia de listas vacías (lo que indicaría una ruta mal definida o carpetas sin imágenes).\n","\n","---\n","\n","## Paso 6: Análisis de distribución por clase (detección de desbalance)\n","Se calcula el conteo de imágenes por clase y se ordena (normalmente de menor a mayor) para identificar:\n","\n","- clases minoritarias,\n","- clases dominantes,\n","- posibles problemas de “clase con pocas muestras”.\n","\n","**Relevancia para Machine Learning**\n","- En clasificación multiclase, el **desbalance** afecta fuertemente:\n","  - la interpretación de *accuracy*,\n","  - el rendimiento en clases minoritarias,\n","  - y la capacidad del modelo para generalizar.\n","- Este diagnóstico justifica decisiones posteriores, por ejemplo:\n","  - ajustar métricas (Macro F1),\n","  - aplicar técnicas de balanceo o augmentación,\n","  - o filtrar clases con muy pocas muestras (si son errores del dataset).\n","\n","---\n","\n","## Resultado del bloque\n","Al finalizar este bloque se obtiene:\n","\n","1. Un **dataset supervisado estructurado** en forma de tabla (`df`) con pares \\((x, y)\\).\n","2. Un conteo por clase que permite evaluar la **calidad y balance** del conjunto de datos.\n","3. Un punto de control crítico para prevenir errores posteriores en el *split* estratificado o en la evaluación.\n"],"metadata":{"id":"qIUVF87xXMQx"}},{"cell_type":"code","source":["# ============================================\n","# Bloque 3: Indexar imágenes por clase\n","# ============================================\n","\n","valid_ext = (\".jpg\", \".jpeg\", \".png\")\n","\n","image_paths, labels = [], []\n","\n","for cls in class_names:\n","    cls_dir = os.path.join(DATA_ROOT, cls)\n","    files = []\n","    for ext in valid_ext:\n","        files.extend(glob.glob(os.path.join(cls_dir, f\"*{ext}\")))\n","    image_paths.extend(files)\n","    labels.extend([cls] * len(files))\n","\n","df = pd.DataFrame({\"path\": image_paths, \"label\": labels})\n","\n","print(\"✅ Total imágenes:\", len(df))\n","display(df.head())\n","\n","counts = df[\"label\"].value_counts().sort_values(ascending=True)\n","print(\"✅ Distribución por clase:\")\n","display(counts)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":402},"id":"P9pChw4sARap","executionInfo":{"status":"ok","timestamp":1768302117231,"user_tz":180,"elapsed":84,"user":{"displayName":"Guido Rios","userId":"10562185620794585819"}},"outputId":"2c5c81b8-1cd6-4735-a324-d1f72ea809b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Total imágenes: 150\n"]},{"output_type":"display_data","data":{"text/plain":["                                                path                 label\n","0  /content/DataImg/TomatoDataset_ready/Tomato___...  Tomato___Late_blight\n","1  /content/DataImg/TomatoDataset_ready/Tomato___...  Tomato___Late_blight\n","2  /content/DataImg/TomatoDataset_ready/Tomato___...  Tomato___Late_blight\n","3  /content/DataImg/TomatoDataset_ready/Tomato___...  Tomato___Late_blight\n","4  /content/DataImg/TomatoDataset_ready/Tomato___...  Tomato___Late_blight"],"text/html":["\n","  <div id=\"df-3e516808-1692-41c0-9bb6-9acc8d80cd92\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>path</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/content/DataImg/TomatoDataset_ready/Tomato___...</td>\n","      <td>Tomato___Late_blight</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/content/DataImg/TomatoDataset_ready/Tomato___...</td>\n","      <td>Tomato___Late_blight</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/content/DataImg/TomatoDataset_ready/Tomato___...</td>\n","      <td>Tomato___Late_blight</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/content/DataImg/TomatoDataset_ready/Tomato___...</td>\n","      <td>Tomato___Late_blight</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/content/DataImg/TomatoDataset_ready/Tomato___...</td>\n","      <td>Tomato___Late_blight</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3e516808-1692-41c0-9bb6-9acc8d80cd92')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-3e516808-1692-41c0-9bb6-9acc8d80cd92 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-3e516808-1692-41c0-9bb6-9acc8d80cd92');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"display(counts)\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"/content/DataImg/TomatoDataset_ready/Tomato___Late_blight/c7871a80-0d29-40ab-b117-344808cb3485___GHLB_PS Leaf 33 Day 13.jpg\",\n          \"/content/DataImg/TomatoDataset_ready/Tomato___Late_blight/f7be50b3-65ad-4d0d-9c7f-55ab2de07b1b___GHLB_PS Leaf 19 Day 8.jpg\",\n          \"/content/DataImg/TomatoDataset_ready/Tomato___Late_blight/d5f68aaa-5700-4957-b08c-dcab8be0d49e___GHLB_PS Leaf 1 Day 9.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Tomato___Late_blight\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✅ Distribución por clase:\n"]},{"output_type":"display_data","data":{"text/plain":["label\n","Tomato___healthy          1\n","Tomato___Late_blight    149\n","Name: count, dtype: int64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","    </tr>\n","    <tr>\n","      <th>label</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Tomato___healthy</th>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>Tomato___Late_blight</th>\n","      <td>149</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> int64</label>"]},"metadata":{}}]},{"cell_type":"markdown","source":["# Bloque 4 — Diagnóstico y filtrado de clases con pocas muestras"],"metadata":{"id":"xkGjHcOxAYSh"}},{"cell_type":"markdown","source":["\n","\n","## Objetivo del bloque\n","Este bloque realiza un **control de calidad del dataset** para asegurar que cada clase tenga suficientes muestras como para permitir:\n","\n","- **partición estratificada** (train/val/test),\n","- **evaluación estable** con métricas por clase,\n","- y **entrenamiento confiable** sin clases “degeneradas”.\n","\n","En clasificación supervisada multiclase, si alguna clase tiene muy pocas imágenes (por ejemplo 1 o 2), se producen dos problemas críticos:\n","\n","1. **Imposibilidad de estratificar correctamente**: el algoritmo de partición no puede garantizar que todas las clases estén representadas en train y test (o val).\n","2. **Métricas inválidas o inestables**: el reporte por clase (precision/recall/F1) pierde sentido estadístico cuando el soporte es extremadamente bajo.\n","\n","---\n","\n","## Paso 1: Diagnóstico del mínimo de muestras por clase\n","Se identifica el tamaño de la clase más pequeña (el mínimo conteo dentro del vector de frecuencias por clase).\n","\n","**Interpretación**\n","- Si el mínimo es **1**, es imposible aplicar `stratify` en un split estándar porque no hay forma de distribuir esa clase en más de un conjunto.\n","- Si el mínimo es bajo (por ejemplo 2–4), incluso si el split se ejecuta, la evaluación será frágil y altamente variable.\n","\n","Este diagnóstico justifica decisiones posteriores de filtrado o recolección de más datos.\n","\n","---\n","\n","## Paso 2: Definición del umbral mínimo por clase (criterio de filtrado)\n","Se fija un parámetro `MIN_PER_CLASS` que determina la cantidad mínima de imágenes requeridas para que una clase permanezca en el experimento.\n","\n","**Justificación metodológica del umbral**\n","- Un valor como **5** es un estándar razonable para permitir particiones múltiples con estratificación (train/val/test).\n","- Mientras más alta la exigencia (por ejemplo 10 o 20), mayor estabilidad estadística, pero también se eliminan más clases o datos.\n","- Este umbral es un compromiso entre **calidad del experimento** y **cobertura del conjunto de clases**.\n","\n","---\n","\n","## Paso 3: Selección de clases válidas según el umbral\n","Se filtra el conjunto de clases, conservando únicamente aquellas cuyo conteo es mayor o igual al mínimo definido.\n","\n","**Resultado conceptual**\n","Se construye el conjunto de clases que tienen suficiente evidencia empírica como para participar en un experimento supervisado válido.\n","\n","---\n","\n","## Paso 4: Filtrado del DataFrame para construir el dataset final\n","Se crea un nuevo dataset (una copia filtrada del DataFrame) que contiene únicamente imágenes pertenecientes a clases válidas.\n","\n","**Por qué se usa una copia**\n","- Evita modificar el dataset original (buena práctica para trazabilidad).\n","- Permite comparar “antes vs después” y auditar qué se eliminó.\n","\n","---\n","\n","## Paso 5: Comparación del dataset antes y después del filtrado\n","Se reporta:\n","\n","- tamaño total del dataset (número de imágenes),\n","- cantidad de clases,\n","antes y después del proceso.\n","\n","**Interpretación**\n","- Permite cuantificar el impacto del filtrado:\n","  - cuántas imágenes se removieron,\n","  - cuántas clases se excluyeron,\n","  - y si el experimento sigue siendo representativo del problema.\n","\n","---\n","\n","## Paso 6: Identificación explícita de clases eliminadas\n","Se calcula y reporta el conjunto de clases que fueron removidas por no cumplir el umbral mínimo.\n","\n","**Relevancia académica**\n","Esto es clave para el informe y el análisis crítico, porque:\n","- documenta decisiones metodológicas,\n","- evita sesgos ocultos (“parece que el modelo funciona bien” porque se eliminaron clases difíciles),\n","- y permite proponer trabajo futuro (p.ej., “recolectar más imágenes para las clases eliminadas”).\n","\n","---\n","\n","## Paso 7: Re-cálculo de la distribución por clase en el dataset filtrado\n","Se vuelve a calcular la distribución por clase usando el dataset filtrado.\n","\n","**Por qué es importante**\n","- Confirma que el filtrado se aplicó correctamente.\n","- Permite verificar si persiste el desbalance (probablemente sí).\n","- Establece el nuevo mínimo por clase, validando que ahora se cumple el criterio para splits estratificados.\n","\n","---\n","\n","## Resultado del bloque\n","Al finalizar este bloque se obtiene un dataset depurado (`df_f`) que:\n","\n","1. **Elimina clases con evidencia insuficiente**, evitando errores de estratificación y métricas inestables.\n","2. Deja una distribución más adecuada para entrenamiento y evaluación.\n","3. Genera trazabilidad explícita para el informe: qué se removió y por qué.\n","\n","Este paso mejora la **validez estadística** del experimento y la **robustez metodológica** del pipeline completo.\n"],"metadata":{"id":"UMgJ2w1lBXO6"}},{"cell_type":"code","source":["# ============================================\n","# Bloque 4: Diagnóstico y filtrado de clases con pocas muestras\n","# ============================================\n","\n","min_count = counts.min()\n","print(\"Mínimo de imágenes en una clase:\", min_count)\n","\n","# Ajusta según tu necesidad:\n","# - 5 es un umbral razonable si quieres train/val/test estratificado.\n","MIN_PER_CLASS = 5\n","\n","valid_classes = counts[counts >= MIN_PER_CLASS].index\n","df_f = df[df[\"label\"].isin(valid_classes)].copy()\n","\n","print(\"\\n✅ Dataset antes:\", df.shape, \" | después:\", df_f.shape)\n","print(\"✅ Clases antes:\", df['label'].nunique(), \" | después:\", df_f['label'].nunique())\n","\n","# Reportar clases eliminadas (si las hay)\n","removed = set(df[\"label\"].unique()) - set(df_f[\"label\"].unique())\n","if removed:\n","    print(\"\\n⚠️ Clases removidas por tener < MIN_PER_CLASS imágenes:\")\n","    print(sorted(list(removed)))\n","\n","# Nueva distribución\n","counts_f = df_f[\"label\"].value_counts().sort_values(ascending=True)\n","print(\"\\n✅ Nueva distribución (filtrada):\")\n","display(counts_f)\n","print(\"Mínimo ahora:\", counts_f.min())\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":320},"id":"14Pw7kSkAa4p","executionInfo":{"status":"ok","timestamp":1768302157958,"user_tz":180,"elapsed":55,"user":{"displayName":"Guido Rios","userId":"10562185620794585819"}},"outputId":"160d6fc7-ed04-4bee-8a34-b06344e38ee4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mínimo de imágenes en una clase: 1\n","\n","✅ Dataset antes: (150, 2)  | después: (149, 2)\n","✅ Clases antes: 2  | después: 1\n","\n","⚠️ Clases removidas por tener < MIN_PER_CLASS imágenes:\n","['Tomato___healthy']\n","\n","✅ Nueva distribución (filtrada):\n"]},{"output_type":"display_data","data":{"text/plain":["label\n","Tomato___Late_blight    149\n","Name: count, dtype: int64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","    </tr>\n","    <tr>\n","      <th>label</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Tomato___Late_blight</th>\n","      <td>149</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> int64</label>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Mínimo ahora: 149\n"]}]},{"cell_type":"markdown","source":["# Bloque 5 — Codificación de etiquetas (*Label Encoding*) + *split* estratificado"],"metadata":{"id":"JtS5msLKBlAB"}},{"cell_type":"markdown","source":["\n","## Objetivo del bloque\n","Este bloque transforma el dataset filtrado (`df_f`) en un conjunto de datos supervisado listo para modelado, realizando dos tareas fundamentales:\n","\n","1. **Codificar las etiquetas de clase** (texto → números) para que sean compatibles con algoritmos de Machine Learning y Deep Learning.\n","2. Construir una partición **estratificada** en tres subconjuntos: **entrenamiento**, **validación** y **prueba**, preservando la proporción relativa de clases en cada subconjunto.\n","\n","Esto asegura una evaluación metodológicamente correcta y comparaciones justas entre modelos.\n","\n","---\n","\n","## Paso 1: Inicialización del codificador de etiquetas\n","Se instancia un codificador de etiquetas (*LabelEncoder*), cuyo propósito es mapear cada clase categórica (por ejemplo, nombres de enfermedades) a un identificador numérico entero.\n","\n","**Motivación**\n","- Los modelos no operan directamente con cadenas de texto como etiquetas.\n","- La codificación crea una representación numérica consistente y reversible:\n","  - clase → índice (entrenamiento/evaluación),\n","  - índice → clase (interpretación de resultados).\n","\n","---\n","\n","## Paso 2: Ajuste del codificador y transformación de etiquetas\n","Se aplica el proceso de *fit + transform* sobre la columna `label`:\n","\n","- **fit**: aprende el conjunto de clases disponibles en el dataset filtrado.\n","- **transform**: asigna un entero a cada clase y genera la variable objetivo numérica `y`.\n","\n","**Resultado**\n","- Se crea una nueva columna `y` en el DataFrame, que corresponde a la **variable objetivo** del problema supervisado.\n","\n","---\n","\n","## Paso 3: Definición formal de variables de entrada y salida\n","Se separan explícitamente:\n","\n","- **X**: variable de entrada, que en este caso corresponde a las **rutas de archivo** de las imágenes.\n","- **y**: variable objetivo, que corresponde a la **clase numérica** (enfermedad/healthy) asociada a cada imagen.\n","\n","**Nota metodológica importante**\n","- Aunque `X` contiene rutas y no pixeles, este enfoque es correcto porque el pipeline posterior cargará las imágenes desde esas rutas de forma eficiente (por lotes) para extraer características o entrenar el modelo.\n","\n","---\n","\n","## Paso 4: Primera partición estratificada (Train/Test 80/20)\n","Se realiza una división en dos conjuntos:\n","\n","- **Train (80%)**: usado para entrenamiento del modelo y ajuste de hiperparámetros.\n","- **Test (20%)**: usado únicamente al final para medir generalización.\n","\n","Se aplica **estratificación** usando `y`, lo que garantiza que la proporción de clases en el conjunto de prueba sea similar a la del dataset original.\n","\n","**Por qué estratificar**\n","- En problemas multiclase con posible desbalance, un split aleatorio puede dejar clases poco representadas (o incluso ausentes) en test.\n","- Estratificar aumenta la validez de las métricas y reduce la varianza del proceso de evaluación.\n","\n","---\n","\n","## Paso 5: Segunda partición estratificada (Validación desde Train)\n","Luego, desde el conjunto de entrenamiento se separa un subconjunto de **validación**.\n","\n","- Se toma una fracción del train equivalente a **12.5%**, lo que produce aproximadamente **10% del total** como validación.\n","- La validación se utiliza para:\n","  - monitorear desempeño durante desarrollo,\n","  - comparar configuraciones,\n","  - realizar verificaciones previas a la evaluación final.\n","\n","Nuevamente se aplica estratificación, esta vez sobre `y_train`, para mantener la distribución de clases dentro de train y val.\n","\n","---\n","\n","## Paso 6: Verificación del resultado del split\n","Se reporta el tamaño de cada conjunto (train/val/test) y el número final de clases.\n","\n","**Relevancia**\n","- Verifica que la partición sea consistente con el diseño experimental.\n","- Confirma cuántas clases sobrevivieron al filtrado previo (`len(le.classes_)`).\n","- Este control facilita detectar errores como:\n","  - splits incorrectos,\n","  - pérdida de clases,\n","  - o inconsistencias en el dataset.\n","\n","---\n","\n","## Resultado del bloque\n","Al finalizar este bloque se obtiene:\n","\n","1. Una variable objetivo numérica (`y`) consistente con las clases del dataset.\n","2. Tres conjuntos estratificados:\n","   - **Train**: para entrenamiento y tuning,\n","   - **Val**: para validación intermedia,\n","   - **Test**: para evaluación final (generalización).\n","3. Un mapeo estable entre índices y nombres de clases (`le.classes_`), crucial para interpretar reportes y matrices de confusión.\n"],"metadata":{"id":"xOI0feASYcLZ"}},{"cell_type":"code","source":["# ============================================\n","# Bloque 5: Label Encoding + split estratificado\n","# ============================================\n","\n","le = LabelEncoder()\n","df_f[\"y\"] = le.fit_transform(df_f[\"label\"])\n","\n","X = df_f[\"path\"].values\n","y = df_f[\"y\"].values\n","\n","# 80/20 train-test\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y,\n","    test_size=0.2,\n","    random_state=SEED,\n","    stratify=y\n",")\n","\n","# Val desde train (12.5% del train = 10% total aprox)\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_train, y_train,\n","    test_size=0.125,\n","    random_state=SEED,\n","    stratify=y_train\n",")\n","\n","print(\"✅ Split completo:\")\n","print(\"Train:\", len(X_train), \"Val:\", len(X_val), \"Test:\", len(X_test))\n","print(\"Clases finales:\", len(le.classes_))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U_XO9yXYBnPC","executionInfo":{"status":"ok","timestamp":1768302217597,"user_tz":180,"elapsed":46,"user":{"displayName":"Guido Rios","userId":"10562185620794585819"}},"outputId":"8bbaa7a1-136b-46f5-bc54-d72e32c75511"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Split completo:\n","Train: 104 Val: 15 Test: 30\n","Clases finales: 1\n"]}]},{"cell_type":"markdown","source":["# Bloque 6 — Construcción del *pipeline* `tf.data`"],"metadata":{"id":"EjT8J67EBqeh"}},{"cell_type":"markdown","source":["\n","\n","## Objetivo del bloque\n","Este bloque implementa un *pipeline* de entrada de datos utilizando la API `tf.data` de TensorFlow, con el propósito de:\n","\n","1. **Cargar imágenes desde disco** a partir de sus rutas (`paths`).\n","2. Aplicar **preprocesamiento básico** (decodificación, redimensionamiento y conversión de tipo).\n","3. Entregar los datos al modelo de manera **eficiente y escalable**, mediante:\n","   - procesamiento en paralelo,\n","   - agrupación por lotes (*batching*),\n","   - y prelectura (*prefetch*) para optimizar el rendimiento en CPU/GPU.\n","\n","El resultado final son tres datasets (`ds_train`, `ds_val`, `ds_test`) listos para entrenamiento y evaluación.\n","\n","---\n","\n","## Paso 1: Definición de hiperparámetros del pipeline (tamaño e intensidad de carga)\n","Se define:\n","\n","- **Tamaño objetivo de imagen**: se estandariza cada imagen a una resolución fija.\n","- **Tamaño de lote (batch size)**: número de imágenes procesadas en conjunto por iteración.\n","\n","### Justificación técnica\n","- Un tamaño fijo de imagen es obligatorio para alimentar una red neuronal de forma consistente.\n","- El tamaño del batch controla el equilibrio entre:\n","  - velocidad (batches más grandes suelen ser más eficientes),\n","  - consumo de memoria (batches grandes pueden saturar la RAM/GPU),\n","  - estabilidad de entrenamiento (aunque aquí el objetivo principal es estandarizar el flujo).\n","\n","En este pipeline, la resolución seleccionada es coherente con modelos preentrenados comunes (por ejemplo, EfficientNetB0).\n","\n","---\n","\n","## Paso 2: Función de carga y preprocesamiento por muestra\n","Se define una función que recibe:\n","- una ruta de archivo (path),\n","- su etiqueta (label),\n","\n","y retorna la imagen ya procesada junto con su etiqueta.\n","\n","Dentro de esta función se realizan operaciones estándar:\n","\n","1. **Lectura del archivo desde disco**: se carga el contenido binario.\n","2. **Decodificación de la imagen**: se interpreta el binario como imagen RGB (3 canales).\n","3. **Redimensionamiento** a un tamaño fijo: esto normaliza la entrada al modelo.\n","4. **Conversión a tipo float**: deja la imagen en un formato numérico compatible con operaciones posteriores (por ejemplo, normalización o *preprocess_input* de un modelo preentrenado).\n","\n","### Relevancia metodológica\n","Separar esta lógica en una función:\n","- hace el flujo modular y reutilizable,\n","- permite mantener consistencia de preprocesamiento entre train/val/test,\n","- y facilita cambios (por ejemplo, agregar augmentación solo en entrenamiento).\n","\n","---\n","\n","## Paso 3: Función constructora del dataset (`make_dataset`)\n","Se define una función que arma el dataset completo a partir de:\n","- una lista/array de rutas,\n","- una lista/array de etiquetas,\n","- y una opción de mezcla aleatoria (*shuffle*).\n","\n","Esta función construye una secuencia de transformaciones:\n","\n","### 3.1 Creación del dataset desde tensores\n","Se construye un dataset base emparejando (ruta, etiqueta) como pares \\((x_i, y_i)\\).\n","\n","**Ventaja**\n","- Permite tratar el flujo como un “stream” de datos, sin cargar todo en memoria como imágenes.\n","\n","### 3.2 Mezcla opcional (*shuffle*)\n","Si se activa, se mezclan las muestras antes de generar los lotes.\n","\n","**Justificación**\n","- En entrenamiento clásico, *shuffle* ayuda a reducir correlaciones y mejorar la generalización.\n","- En este notebook, mantener `shuffle=False` puede ser útil cuando el objetivo es solo extraer embeddings de manera determinista.\n","- En escenarios de entrenamiento de redes, normalmente se recomienda `shuffle=True` solo para el conjunto de entrenamiento.\n","\n","### 3.3 Mapeo del preprocesamiento (`map`)\n","Se aplica la función de carga y preprocesamiento a cada elemento, habilitando procesamiento paralelo.\n","\n","**Justificación**\n","- El parámetro de paralelización permite aprovechar múltiples hilos CPU.\n","- Mejora significativamente el rendimiento cuando hay muchas imágenes.\n","\n","### 3.4 Agrupación en lotes (`batch`)\n","Se agrupan muestras para que el modelo procese múltiples imágenes por iteración.\n","\n","**Justificación**\n","- Reduce overhead de llamadas al modelo.\n","- Aumenta eficiencia computacional, especialmente con GPU.\n","\n","### 3.5 Prelectura (`prefetch`)\n","Se habilita prelectura asincrónica de batches.\n","\n","**Por qué es importante**\n","- Mientras la GPU procesa un batch, el siguiente se prepara en paralelo.\n","- Reduce tiempos muertos de entrenamiento/evaluación (pipeline más “fluido”).\n","\n","---\n","\n","## Paso 4: Construcción de `ds_train`, `ds_val` y `ds_test`\n","Se aplica `make_dataset` a los tres subconjuntos resultantes del *split*:\n","\n","- **Train**: datos para ajustar el modelo.\n","- **Validation**: datos para monitoreo y decisiones metodológicas.\n","- **Test**: datos reservados para evaluación final de generalización.\n","\n","### Importancia metodológica\n","Separar estos datasets es clave para:\n","- evitar *data leakage*,\n","- obtener métricas confiables,\n","- y cumplir buenas prácticas de evaluación supervisada.\n","\n","---\n","\n","## Resultado del bloque\n","Al finalizar se obtiene un pipeline `tf.data` listo y eficiente que:\n","\n","1. Carga imágenes desde sus rutas sin saturar memoria.\n","2. Estandariza el tamaño y formato de entrada.\n","3. Acelera el flujo mediante paralelización y prefetch.\n","4. Entrega datasets separados (train/val/test) consistentes con un diseño experimental riguroso.\n"],"metadata":{"id":"r5S-X0WZYspR"}},{"cell_type":"code","source":["# ============================================\n","# Bloque 6: tf.data pipeline\n","# ============================================\n","\n","IMG_SIZE = (224, 224)\n","BATCH_SIZE = 64\n","\n","def load_and_preprocess(path, label):\n","    img = tf.io.read_file(path)\n","    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n","    img = tf.image.resize(img, IMG_SIZE)\n","    img = tf.cast(img, tf.float32)\n","    return img, label\n","\n","def make_dataset(paths, labels, shuffle=False):\n","    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n","    if shuffle:\n","        ds = ds.shuffle(buffer_size=len(paths), seed=SEED, reshuffle_each_iteration=True)\n","    ds = ds.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n","    ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n","    return ds\n","\n","ds_train = make_dataset(X_train, y_train, shuffle=False)\n","ds_val   = make_dataset(X_val, y_val, shuffle=False)\n","ds_test  = make_dataset(X_test, y_test, shuffle=False)\n","\n","print(\"✅ tf.data listo\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ok_DH45-BtIh","executionInfo":{"status":"ok","timestamp":1768302239461,"user_tz":180,"elapsed":99,"user":{"displayName":"Guido Rios","userId":"10562185620794585819"}},"outputId":"40344ee4-dee1-42d9-cdfb-c8422d557034"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ tf.data listo\n"]}]},{"cell_type":"markdown","source":["# Bloque 7 — Extractor de *embeddings* mediante *Transfer Learning*"],"metadata":{"id":"FyWJxcVoBvsZ"}},{"cell_type":"markdown","source":["\n","\n","## Objetivo del bloque\n","Este bloque transforma cada imagen del dataset en un **vector numérico de características** (*embedding*) utilizando una red neuronal convolucional (CNN) preentrenada.  \n","El propósito es construir una representación compacta y semánticamente útil de las imágenes para posteriormente aplicar un modelo de Machine Learning clásico (por ejemplo, **KNN**) sobre estos vectores.\n","\n","En términos generales, el bloque implementa el esquema:\n","\n","**Imagen (pixeles) → CNN preentrenada (sin entrenamiento) → Embedding (vector) → Modelo ML**\n","\n","---\n","\n","## Paso 1: Selección de un modelo preentrenado y su función en el pipeline\n","Se emplea una CNN conocida por su buen rendimiento en visión por computador (EfficientNetB0), entrenada previamente sobre **ImageNet**, un conjunto masivo y diverso de imágenes.\n","\n","### Justificación teórica (Transfer Learning)\n","- ImageNet permite que la red aprenda patrones visuales generales (bordes, texturas, manchas, formas).\n","- Estos patrones suelen ser transferibles a tareas nuevas, como identificación de enfermedades foliares.\n","- En lugar de entrenar una red desde cero (costoso en datos y cómputo), se reutiliza el conocimiento aprendido como **extractor de características**.\n","\n","---\n","\n","## Paso 2: Configuración de la red como extractor (sin “cabeza” de clasificación)\n","La red se instancia con tres configuraciones clave:\n","\n","1. **`weights=\"imagenet\"`**  \n","   Indica que se utilizan pesos ya entrenados.\n","\n","2. **`include_top=False`**  \n","   Se elimina la capa final original diseñada para clasificar ImageNet (1000 clases).  \n","   Esto es fundamental porque nuestro problema tiene clases distintas (enfermedades del tomate).\n","\n","3. **`pooling=\"avg\"`**  \n","   Se aplica *Global Average Pooling* al final del modelo para convertir los mapas de activación en un **vector** de tamaño fijo.  \n","   Esto produce un embedding compacto que:\n","   - resume la información visual relevante,\n","   - y es adecuado para modelos basados en distancia o modelos lineales.\n","\n","**Resultado conceptual:** la CNN se convierte en una función:\n","\\[\n","f(\\text{imagen}) \\rightarrow \\mathbb{R}^d\n","\\]\n","donde \\( d \\) es la dimensión del embedding.\n","\n","---\n","\n","## Paso 3: Congelamiento de parámetros (no entrenamiento)\n","Se desactiva el entrenamiento de la red base.\n","\n","### Implicancia metodológica\n","- La CNN no ajusta sus pesos con el dataset de tomate.\n","- Se utiliza únicamente como una transformación fija: “de imagen a embedding”.\n","- Esto reduce:\n","  - tiempo de entrenamiento,\n","  - riesgo de sobreajuste con datasets pequeños,\n","  - y complejidad computacional.\n","\n","---\n","\n","## Paso 4: Definición de una función optimizada para obtener embeddings por batch\n","Se define una función que procesa un lote de imágenes y devuelve los embeddings.\n","\n","Dentro de esta función se aplican dos operaciones esenciales:\n","\n","1. **Preprocesamiento específico del modelo (`preprocess_input`)**  \n","   Ajusta escala y distribución de pixeles para que la entrada sea compatible con lo que EfficientNet espera.  \n","   Esto es crítico porque los modelos preentrenados requieren una normalización consistente con su entrenamiento original.\n","\n","2. **Inferencia en modo no-entrenamiento**  \n","   Se fuerza el modo de inferencia para garantizar comportamiento estable (por ejemplo, evitando efectos de capas que se comportan distinto en entrenamiento e inferencia).\n","\n","Además, la función se marca para ejecución optimizada por TensorFlow, lo que puede mejorar el rendimiento al ejecutar repetidamente sobre muchos batches.\n","\n","---\n","\n","## Paso 5: Extracción de embeddings para un dataset completo (train/val/test)\n","Se define una rutina que recorre un `tf.data.Dataset` por lotes y construye:\n","\n","- una matriz de embeddings (features),\n","- un vector de etiquetas correspondientes.\n","\n","### Qué ocurre en cada iteración del dataset\n","Para cada batch:\n","1. Se obtienen las imágenes y sus etiquetas.\n","2. Se calculan los embeddings del batch con la CNN preentrenada.\n","3. Se almacenan embeddings y etiquetas en contenedores acumulativos.\n","\n","Al finalizar:\n","- los embeddings se concatenan en una única matriz:\n","  \\[\n","  X \\in \\mathbb{R}^{n \\times d}\n","  \\]\n","- las etiquetas se concatenan en un vector:\n","  \\[\n","  y \\in \\{0, 1, \\dots, K-1\\}^{n}\n","  \\]\n","\n","---\n","\n","## Paso 6: Construcción final de conjuntos embebidos (train/val/test)\n","Se aplica la extracción a cada partición:\n","\n","- **Train embeddings**: se usarán para entrenar y ajustar el modelo de ML.\n","- **Validation embeddings**: se usan para validación intermedia (si corresponde).\n","- **Test embeddings**: se reservan para evaluación final de generalización.\n","\n","### Importancia metodológica\n","Mantener la separación train/val/test también en el espacio de embeddings evita fuga de información y asegura una evaluación justa.\n","\n","---\n","\n","## Paso 7: Verificación de dimensiones como control de calidad\n","Se imprime la forma (*shape*) de los embeddings generados en cada conjunto.\n","\n","**Por qué esto es clave**\n","- Confirma que el pipeline produce vectores de dimensión fija.\n","- Verifica consistencia entre train/val/test.\n","- Permite detectar errores tempranos (por ejemplo, embeddings vacíos, batches mal cargados o discrepancias de tamaño).\n","\n","---\n","\n","## Resultado del bloque\n","Al terminar este bloque se obtiene:\n","\n","1. Una representación numérica de las imágenes en forma de embeddings.\n","2. Tres matrices listas para Machine Learning clásico:\n","   - \\(X_{\\text{train}}, X_{\\text{val}}, X_{\\text{test}}\\)\n","3. Un puente metodológico sólido entre Deep Learning y ML clásico:\n","   - **Deep Learning** aporta extracción de características,\n","   - **KNN** (u otros modelos) realiza la clasificación en el espacio de embeddings.\n","\n","Este enfoque es especialmente apropiado cuando se busca usar KNN de forma efectiva en imágenes, mitigando la alta dimensionalidad de los pixeles crudos y mejorando la separabilidad entre clases.\n"],"metadata":{"id":"N42k-67tZF4J"}},{"cell_type":"code","source":["# ============================================\n","# Bloque 7: Extractor de embeddings (Transfer Learning)\n","# ============================================\n","\n","from tensorflow.keras.applications import EfficientNetB0\n","from tensorflow.keras.applications.efficientnet import preprocess_input\n","\n","base = EfficientNetB0(\n","    weights=\"imagenet\",\n","    include_top=False,\n","    pooling=\"avg\",\n","    input_shape=(224, 224, 3)\n",")\n","base.trainable = False\n","\n","@tf.function\n","def embed_batch(img_batch):\n","    x = preprocess_input(img_batch)\n","    return base(x, training=False)\n","\n","def extract_embeddings(ds):\n","    feats, labs = [], []\n","    for batch_imgs, batch_labels in ds:\n","        emb = embed_batch(batch_imgs)\n","        feats.append(emb.numpy())\n","        labs.append(batch_labels.numpy())\n","    return np.vstack(feats), np.concatenate(labs)\n","\n","Xtr_emb, ytr = extract_embeddings(ds_train)\n","Xva_emb, yva = extract_embeddings(ds_val)\n","Xte_emb, yte = extract_embeddings(ds_test)\n","\n","print(\"✅ Embeddings generados:\")\n","print(\"Train:\", Xtr_emb.shape, \"Val:\", Xva_emb.shape, \"Test:\", Xte_emb.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1YLS11NjBzAx","executionInfo":{"status":"ok","timestamp":1768302276958,"user_tz":180,"elapsed":14407,"user":{"displayName":"Guido Rios","userId":"10562185620794585819"}},"outputId":"a5023bad-f4a0-47fe-c259-6f32e0b80eb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n","\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n","✅ Embeddings generados:\n","Train: (104, 1280) Val: (15, 1280) Test: (30, 1280)\n"]}]},{"cell_type":"markdown","source":["# Bloque 8 — Entrenamiento de KNN con ajuste riguroso de hiperparámetros (*GridSearchCV*)"],"metadata":{"id":"5FwnLPdMB-wp"}},{"cell_type":"markdown","source":["\n","\n","## Objetivo del bloque\n","Este bloque implementa el **entrenamiento y ajuste sistemático** de un clasificador **K-Nearest Neighbors (KNN)** utilizando como entrada los **embeddings** previamente extraídos de las imágenes.  \n","El objetivo principal es encontrar la combinación de hiperparámetros que **maximice el desempeño de generalización** mediante:\n","\n","- un proceso de búsqueda exhaustiva (*grid search*),\n","- validación cruzada estratificada (*Stratified K-Fold*),\n","- y una métrica robusta para multiclase desbalanceado (**F1-macro**).\n","\n","El resultado final es un modelo KNN optimizado (`best_knn`) listo para evaluación en el conjunto de prueba.\n","\n","---\n","\n","## Paso 1: Construcción de un *pipeline* (escalamiento + KNN)\n","Se define un pipeline que encadena dos etapas:\n","\n","1. **Estandarización de características**\n","2. **Clasificación con KNN**\n","\n","### Justificación del escalamiento (por qué es crítico en KNN)\n","KNN se basa en la distancia entre puntos en el espacio de características.  \n","Si las variables tienen distintas escalas, la distancia queda dominada por las dimensiones con mayor magnitud, provocando un sesgo artificial en la noción de “vecindad”.\n","\n","El escalamiento:\n","- centra y normaliza cada dimensión,\n","- garantiza que cada componente del embedding contribuya de forma comparable,\n","- y mejora la estabilidad del método basado en distancias.\n","\n","### Beneficio metodológico del pipeline\n","- Evita *data leakage*: el escalador se ajusta **solo con train** dentro de cada fold.\n","- Asegura reproducibilidad y consistencia al evaluar múltiples configuraciones.\n","- Facilita extender el flujo con otros modelos manteniendo el mismo preprocesamiento.\n","\n","---\n","\n","## Paso 2: Definición del espacio de búsqueda de hiperparámetros (*param_grid*)\n","Se define una grilla (conjunto discreto) de combinaciones a evaluar. Incluye tres hiperparámetros fundamentales:\n","\n","### 2.1 Número de vecinos \\(k\\)\n","- Controla la complejidad del clasificador.\n","- \\(k\\) pequeño:\n","  - frontera de decisión más irregular,\n","  - mayor sensibilidad al ruido (*alta varianza*).\n","- \\(k\\) grande:\n","  - frontera más suave,\n","  - riesgo de subajuste (*alto sesgo*).\n","\n","Este parámetro representa el compromiso clásico **sesgo–varianza** en KNN.\n","\n","### 2.2 Esquema de ponderación de vecinos\n","- **Uniforme**: todos los vecinos votan con el mismo peso.\n","- **Distancia**: vecinos más cercanos pesan más.\n","\n","**Interpretación**\n","- “distance” puede ser ventajoso cuando:\n","  - existen clases cercanas en el espacio de embeddings,\n","  - pero la proximidad fina aporta información relevante,\n","  - o hay densidades distintas por clase.\n","\n","### 2.3 Métrica de distancia\n","Se comparan diferentes nociones de cercanía, tales como:\n","- Euclidiana (L2),\n","- Manhattan (L1),\n","- y una formulación general (Minkowski).\n","\n","**Relevancia**\n","El rendimiento de KNN depende fuertemente de la geometría del espacio; en embeddings, cambiar la métrica puede alterar significativamente qué puntos son “vecinos”.\n","\n","---\n","\n","## Paso 3: Diseño de validación cruzada estratificada (*StratifiedKFold*)\n","Se configura una validación cruzada en \\(K\\) particiones (folds), asegurando que cada fold conserve proporciones similares de clases.\n","\n","### Justificación\n","- En multiclase con desbalance, folds no estratificados pueden:\n","  - excluir clases minoritarias en algún fold,\n","  - producir métricas inestables,\n","  - y sesgar la selección de hiperparámetros.\n","- La estratificación incrementa la **validez** y reduce la varianza de la estimación del rendimiento.\n","\n","La mezcla aleatoria controlada por semilla mejora la representatividad y reproducibilidad.\n","\n","---\n","\n","## Paso 4: Configuración de GridSearchCV (búsqueda exhaustiva + evaluación)\n","Se crea un objeto de búsqueda que:\n","\n","1. Entrena el pipeline para cada combinación de hiperparámetros.\n","2. Evalúa cada combinación mediante validación cruzada.\n","3. Selecciona la combinación con mejor desempeño promedio según la métrica definida.\n","\n","### Métrica utilizada: F1-macro\n","El bloque utiliza **F1-macro** como criterio principal.\n","\n","**Por qué F1-macro es adecuada aquí**\n","- Calcula F1 por clase y luego promedia asignando **igual peso a cada clase**.\n","- Es robusta ante desbalance: una clase minoritaria mal clasificada reduce el puntaje, incluso si la accuracy global parece alta.\n","- Es apropiada cuando el objetivo es desempeño equilibrado entre enfermedades, no solo “ganar” en las clases frecuentes.\n","\n","---\n","\n","## Paso 5: Entrenamiento del proceso de búsqueda sobre embeddings de entrenamiento\n","Se ajusta la grilla usando los embeddings del conjunto de entrenamiento:\n","\n","- Cada combinación se evalúa en varios folds.\n","- Dentro de cada fold:\n","  - el escalador se ajusta usando solo el subconjunto de entrenamiento del fold,\n","  - el modelo KNN se entrena,\n","  - y se evalúa en la partición de validación del fold.\n","\n","**Resultado conceptual**\n","Se obtiene una estimación del rendimiento promedio y más estable para cada configuración.\n","\n","---\n","\n","## Paso 6: Reporte de la mejor configuración encontrada\n","Una vez completada la búsqueda, se recuperan:\n","\n","- los hiperparámetros que maximizan el F1-macro promedio,\n","- el valor de ese F1-macro como estimación de desempeño en validación cruzada.\n","\n","Esto permite justificar en el informe:\n","- qué configuración se eligió,\n","- por qué se eligió (criterio cuantitativo),\n","- y qué tan consistente fue su desempeño.\n","\n","---\n","\n","## Paso 7: Selección del mejor modelo entrenado (listo para test)\n","Finalmente, se extrae el estimador óptimo como el modelo final.\n","\n","**Importancia**\n","- Este modelo representa la mejor configuración según evidencia empírica controlada (CV).\n","- Queda preparado para el siguiente paso metodológico correcto: **evaluación final en test**, que debe realizarse una sola vez para reportar generalización.\n","\n","---\n","\n","## Resultado del bloque\n","Al finalizar este bloque se obtiene:\n","\n","1. Un procedimiento de selección de hiperparámetros riguroso y reproducible.\n","2. La mejor configuración de KNN según validación cruzada estratificada.\n","3. Un modelo final (`best_knn`) entrenado bajo buenas prácticas, defendible en un contexto académico y alineado con evaluación robusta para multiclase desbalanceado.\n"],"metadata":{"id":"YRfSpgMaZ4AA"}},{"cell_type":"code","source":["# ============================================\n","# Bloque 8: KNN + GridSearchCV\n","# ============================================\n","\n","pipe = Pipeline(steps=[\n","    (\"scaler\", StandardScaler()),        # crítico para distancias en KNN\n","    (\"knn\", KNeighborsClassifier())\n","])\n","\n","param_grid = {\n","    \"knn__n_neighbors\": [3, 5, 7, 9, 11],\n","    \"knn__weights\": [\"uniform\", \"distance\"],\n","    \"knn__metric\": [\"minkowski\", \"euclidean\", \"manhattan\"]\n","}\n","\n","cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n","\n","grid = GridSearchCV(\n","    estimator=pipe,\n","    param_grid=param_grid,\n","    scoring=\"f1_macro\",   # multiclase robusta al desbalance\n","    cv=cv,\n","    n_jobs=-1,\n","    verbose=1\n",")\n","\n","grid.fit(Xtr_emb, ytr)\n","\n","print(\"✅ Mejor KNN encontrado:\")\n","print(\"Mejores params:\", grid.best_params_)\n","print(\"Mejor F1_macro (CV):\", grid.best_score_)\n","\n","best_knn = grid.best_estimator_\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zR9SmUsqB92B","executionInfo":{"status":"ok","timestamp":1768302326799,"user_tz":180,"elapsed":2564,"user":{"displayName":"Guido Rios","userId":"10562185620794585819"}},"outputId":"200546a0-d793-4d90-e56d-3832ea58c4dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 30 candidates, totalling 150 fits\n","✅ Mejor KNN encontrado:\n","Mejores params: {'knn__metric': 'minkowski', 'knn__n_neighbors': 3, 'knn__weights': 'uniform'}\n","Mejor F1_macro (CV): 1.0\n"]}]},{"cell_type":"markdown","source":["# Bloque 9 — Evaluación final en el conjunto de prueba (Test)"],"metadata":{"id":"hxqhmTRdCIga"}},{"cell_type":"markdown","source":["\n","\n","## Objetivo del bloque\n","Este bloque realiza la **evaluación final de generalización** del modelo seleccionado (`best_knn`) utilizando el conjunto **Test**, el cual se mantuvo separado durante el entrenamiento y la selección de hiperparámetros.\n","\n","El propósito es reportar un desempeño **honesto y defendible**, midiendo cómo se comporta el clasificador frente a datos **no vistos** durante el ajuste del modelo.\n","\n","---\n","\n","## Paso 1: Generación de predicciones sobre embeddings de Test\n","Se utiliza el modelo KNN optimizado para predecir la clase de cada instancia del conjunto de prueba a partir de los **embeddings** (`Xte_emb`).\n","\n","**Interpretación**\n","- Cada embedding representa una imagen en un espacio de características aprendido por la CNN preentrenada.\n","- El KNN asigna la clase según los vecinos más cercanos en dicho espacio, aplicando la configuración encontrada en GridSearchCV.\n","\n","El resultado de este paso es un vector `y_pred` con las clases predichas para cada imagen del test.\n","\n","---\n","\n","## Paso 2: Cálculo de métricas globales (Accuracy y F1-macro)\n","\n","### 2.1 Accuracy\n","Se calcula la **exactitud** como la proporción total de predicciones correctas:\n","\n","\\[\n","\\text{Accuracy}=\\frac{\\#\\text{predicciones correctas}}{\\#\\text{muestras totales}}\n","\\]\n","\n","**Ventaja**\n","- Es fácil de interpretar como porcentaje de aciertos globales.\n","\n","**Limitación**\n","- En problemas desbalanceados puede ser engañosa: un modelo puede lograr alta accuracy si domina clases frecuentes, aunque falle en clases minoritarias.\n","\n","### 2.2 F1-macro\n","Se calcula el **F1-macro**, que primero computa el F1-score por clase y luego promedia asignando **el mismo peso a cada clase**:\n","\n","\\[\n","F1_{\\text{macro}}=\\frac{1}{K}\\sum_{k=1}^{K}F1_k\n","\\]\n","\n","**Por qué es una métrica prioritaria aquí**\n","- Penaliza con fuerza el mal desempeño en clases minoritarias.\n","- Es más representativa cuando el objetivo es un rendimiento equilibrado entre enfermedades.\n","\n","---\n","\n","## Paso 3: Reporte de métricas globales formateadas\n","Se imprimen las métricas con formato de cuatro decimales para facilitar:\n","- comparación entre modelos,\n","- inclusión directa en el informe,\n","- y lectura rápida durante la presentación.\n","\n","---\n","\n","## Paso 4: Reconstrucción de nombres de clase para interpretación\n","Se generan los nombres de clases en el orden correcto a partir del codificador de etiquetas (`LabelEncoder`).\n","\n","**Importancia metodológica**\n","- Las métricas por clase deben reportarse usando nombres humanos (por ejemplo, `Tomato___Late_blight`) en lugar de índices numéricos.\n","- Garantiza coherencia entre:\n","  - el índice interno de la clase,\n","  - y el nombre reportado en el informe.\n","\n","---\n","\n","## Paso 5: Reporte de clasificación por clase (*classification report*)\n","Se imprime un informe detallado que incluye, para cada clase:\n","\n","- **Precisión (precision):** de las predicciones hechas como esa clase, cuántas fueron correctas.\n","- **Recall (exhaustividad):** de las instancias reales de esa clase, cuántas detectó el modelo.\n","- **F1-score:** balance entre precisión y recall (media armónica).\n","- **Support:** número real de ejemplos de esa clase en el test.\n","\n","**Valor analítico**\n","Este reporte permite identificar clases donde el modelo:\n","- es **conservador** (alta precisión, bajo recall),\n","- o es **agresivo** (alto recall, baja precisión),\n","y detectar enfermedades específicas con bajo desempeño.\n","\n","---\n","\n","## Paso 6: Matriz de confusión (*confusion matrix*)\n","Se construye e imprime la matriz de confusión, donde:\n","\n","- La **diagonal principal** representa aciertos por clase.\n","- Los valores **fuera de la diagonal** representan confusiones sistemáticas entre clases.\n","\n","**Interpretación práctica**\n","- Permite detectar patrones típicos de error, por ejemplo:\n","  - confusión entre enfermedades visualmente similares,\n","  - sesgos hacia clases con mayor frecuencia,\n","  - o errores consistentes en ciertas clases minoritarias.\n","\n","La matriz es especialmente útil para el **análisis crítico** requerido por la rúbrica, ya que aporta evidencia concreta para proponer mejoras (más datos, augmentación, ajuste del extractor, etc.).\n","\n","---\n","\n","## Resultado del bloque\n","Al finalizar, se obtiene una evaluación completa y defendible del modelo en datos no vistos:\n","\n","1. **Test Accuracy** (visión global de aciertos).\n","2. **Test F1-macro** (métrica robusta para multiclase desbalanceado).\n","3. **Reporte por clase** (diagnóstico fino de precisión/recall/F1).\n","4. **Matriz de confusión** (análisis detallado de patrones de error).\n","\n","Este conjunto de salidas permite comparar modelos, sustentar conclusiones y construir una discusión crítica alineada con estándares académicos y con la rúbrica del proyecto.\n"],"metadata":{"id":"ppiPxGJZaOtI"}},{"cell_type":"code","source":["# ============================================\n","# Bloque 9: Evaluación final en Test\n","# ============================================\n","\n","y_pred = best_knn.predict(Xte_emb)\n","\n","acc = accuracy_score(yte, y_pred)\n","f1m = f1_score(yte, y_pred, average=\"macro\")\n","\n","print(f\"✅ Test Accuracy : {acc:.4f}\")\n","print(f\"✅ Test F1-macro : {f1m:.4f}\\n\")\n","\n","target_names = le.inverse_transform(np.arange(len(le.classes_)))\n","print(classification_report(yte, y_pred, target_names=target_names))\n","\n","cm = confusion_matrix(yte, y_pred)\n","print(\"Matriz de confusión:\\n\", cm)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0uYoWi9cCLiC","executionInfo":{"status":"ok","timestamp":1768302362622,"user_tz":180,"elapsed":67,"user":{"displayName":"Guido Rios","userId":"10562185620794585819"}},"outputId":"9ae43fdb-5481-4234-83e0-02bc06658226"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Test Accuracy : 1.0000\n","✅ Test F1-macro : 1.0000\n","\n","                      precision    recall  f1-score   support\n","\n","Tomato___Late_blight       1.00      1.00      1.00        30\n","\n","            accuracy                           1.00        30\n","           macro avg       1.00      1.00      1.00        30\n","        weighted avg       1.00      1.00      1.00        30\n","\n","Matriz de confusión:\n"," [[30]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["# Bloque 10 — Inferencia interactiva: subir una imagen y predecir"],"metadata":{"id":"Yq4oLWxCCOMZ"}},{"cell_type":"markdown","source":["\n","\n","## Objetivo del bloque\n","Este bloque implementa un flujo de **inferencia en tiempo real** dentro de Google Colab, permitiendo que el usuario:\n","\n","1. **Suba una imagen nueva** (no necesariamente perteneciente al dataset).\n","2. Aplique el mismo preprocesamiento usado en el entrenamiento.\n","3. Genere su **embedding** con el extractor basado en *Transfer Learning*.\n","4. Use el clasificador **KNN** ya entrenado para:\n","   - predecir la clase más probable,\n","   - y obtener un ranking **Top-5** con probabilidades asociadas.\n","\n","Este bloque representa una aproximación conceptual a un **despliegue** (*deployment*) en un entorno real: entrada → modelo → salida.\n","\n","---\n","\n","## Paso 1: Importación de herramientas para interacción en Colab\n","Se cargan utilidades propias del entorno Google Colab y de Keras para:\n","\n","- permitir la **carga manual de archivos** desde el computador del usuario,\n","- leer y transformar imágenes en arreglos numéricos compatibles con el pipeline.\n","\n","**Relevancia**\n","Este paso hace que el notebook no sea solo “entrenamiento”, sino también una herramienta práctica de uso, alineada con la sección de despliegue conceptual del proyecto.\n","\n","---\n","\n","## Paso 2: Subida de imagen(s) por parte del usuario\n","Se habilita un mecanismo interactivo para seleccionar archivos locales.  \n","El resultado es una colección de archivos cargados, que puede contener una o varias imágenes.\n","\n","**Interpretación**\n","- Cada archivo cargado representa una nueva instancia \\(x_{\\text{nuevo}}\\).\n","- Estas imágenes no requieren estar dentro de la estructura de carpetas por clase.\n","\n","---\n","\n","## Paso 3: Definición de una función reutilizable de predicción\n","Se define una función que encapsula el proceso completo de inferencia para una sola imagen, lo cual es una buena práctica porque:\n","\n","- hace el flujo modular y reutilizable,\n","- permite evaluar múltiples imágenes con la misma metodología,\n","- evita duplicación de código,\n","- y facilita su futura migración a una API o aplicación.\n","\n","---\n","\n","## Paso 4: Carga de la imagen y estandarización del formato de entrada\n","Dentro de la función:\n","\n","1. La imagen se carga desde disco.\n","2. Se redimensiona al tamaño esperado por el pipeline (**mismo tamaño usado en el extractor**).\n","3. Se convierte a un arreglo numérico (tensor/array).\n","\n","**Justificación**\n","Los modelos preentrenados y los extractores de embeddings exigen un tamaño de entrada fijo.  \n","Además, convertir a arreglo numérico es necesario porque el modelo trabaja con tensores, no con archivos.\n","\n","---\n","\n","## Paso 5: Preparación del batch (dimensión adicional)\n","Se añade una dimensión extra para representar el tamaño del lote (*batch*), aun cuando se prediga una sola imagen.\n","\n","**Por qué es necesario**\n","- TensorFlow/Keras espera entradas con forma:\n","  \\[\n","  (batch\\_size, height, width, channels)\n","  \\]\n","- Esto permite que el mismo pipeline funcione tanto para una imagen como para cientos en lote.\n","\n","---\n","\n","## Paso 6: Extracción del embedding con el modelo preentrenado\n","La imagen procesada se pasa por la función de embeddings.\n","\n","**Interpretación conceptual**\n","- La CNN actúa como una transformación:\n","  \\[\n","  \\text{imagen} \\rightarrow \\text{vector de características}\n","  \\]\n","- Ese vector representa una descripción compacta (texturas, manchas, estructuras) útil para comparar similitud entre hojas.\n","\n","Este paso es el puente entre Deep Learning (representación) y KNN (clasificación basada en vecindad).\n","\n","---\n","\n","## Paso 7: Predicción de clase con KNN\n","Con el embedding generado:\n","\n","1. Se obtiene la **clase predicha** (la más probable según KNN).\n","2. Se obtienen las **probabilidades por clase**.\n","\n","**Interpretación**\n","- En KNN, las probabilidades suelen derivarse de la proporción de vecinos por clase (o ponderaciones), por lo que:\n","  - reflejan “qué tan apoyada” está la decisión por ejemplos cercanos,\n","  - y entregan una medida práctica de confianza relativa.\n","\n","---\n","\n","## Paso 8: Decodificación de la clase predicha a su nombre original\n","Como el modelo trabaja con etiquetas numéricas internas, se reconstruye el nombre de clase original (por ejemplo, `Tomato___Leaf_Mold`).\n","\n","**Importancia**\n","- Hace la salida interpretable para el usuario final.\n","- Permite que la herramienta sea utilizable sin conocer la codificación numérica.\n","\n","---\n","\n","## Paso 9: Construcción del ranking Top-5\n","Se ordenan las probabilidades de mayor a menor y se seleccionan las 5 clases más probables.\n","\n","**Valor práctico**\n","- El Top-5 es útil cuando:\n","  - hay clases visualmente similares,\n","  - el modelo tiene incertidumbre,\n","  - se busca apoyar a un experto humano con alternativas plausibles.\n","- Favorece el análisis crítico: si el Top-1 falla, se puede observar si la respuesta correcta estaba en el Top-5.\n","\n","---\n","\n","## Paso 10: Inferencia sobre todas las imágenes cargadas\n","Finalmente, se recorre cada archivo subido y se ejecuta el proceso de predicción, mostrando:\n","\n","- la clase predicha (Top-1),\n","- y el ranking Top-5 con probabilidades.\n","\n","**Resultado operacional**\n","Este bloque convierte el notebook en una herramienta interactiva de predicción, alineada con un flujo de despliegue conceptual:\n","\n","**Entrada (imagen) → Embedding → Clasificador KNN → Salida (predicción + ranking)**\n","\n","---\n","\n","## Resultado del bloque\n","Al finalizar este bloque, se obtiene:\n","\n","1. Un mecanismo funcional para **probar el modelo con imágenes nuevas**.\n","2. Predicciones interpretables en términos de nombres de enfermedades.\n","3. Un ranking Top-5 que aporta:\n","   - interpretabilidad práctica,\n","   - manejo de incertidumbre,\n","   - y evidencia para discusión metodológica y de despliegue.\n"],"metadata":{"id":"2UDIP_e0bh6h"}},{"cell_type":"code","source":["# ============================================\n","# Bloque 10: Inferencia (subir imagen y predecir)\n","# ============================================\n","\n","from google.colab import files\n","from tensorflow.keras.preprocessing import image as kimage\n","\n","uploaded = files.upload()\n","\n","def predict_image(path):\n","    img = kimage.load_img(path, target_size=IMG_SIZE)\n","    arr = kimage.img_to_array(img)\n","    arr = np.expand_dims(arr, axis=0).astype(np.float32)\n","\n","    emb = embed_batch(arr).numpy()\n","    pred = best_knn.predict(emb)[0]\n","    proba = best_knn.predict_proba(emb)[0]\n","\n","    pred_label = le.inverse_transform([pred])[0]\n","    top5_idx = np.argsort(proba)[::-1][:5]\n","    top5 = [(le.inverse_transform([i])[0], float(proba[i])) for i in top5_idx]\n","\n","    return pred_label, top5\n","\n","for fname in uploaded.keys():\n","    label, top5 = predict_image(fname)\n","    print(\"✅ Predicción:\", label)\n","    print(\"Top-5:\")\n","    for cls, p in top5:\n","        print(f\"  - {cls}: {p:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":125},"id":"VTygVukcCUYB","executionInfo":{"status":"ok","timestamp":1768302449443,"user_tz":180,"elapsed":5908,"user":{"displayName":"Guido Rios","userId":"10562185620794585819"}},"outputId":"880d6270-a9dc-43d9-be22-123b07575e14"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-80bed0e3-e891-4b02-ac54-b8bbe561799f\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-80bed0e3-e891-4b02-ac54-b8bbe561799f\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving 01425d17-4c97-46e3-b395-c1453b78ab78___GHLB2 Leaf 9100.JPG to 01425d17-4c97-46e3-b395-c1453b78ab78___GHLB2 Leaf 9100.JPG\n","✅ Predicción: Tomato___Late_blight\n","Top-5:\n","  - Tomato___Late_blight: 1.0000\n"]}]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOYzVg4vo8ukIVLbDFZgxtt"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}